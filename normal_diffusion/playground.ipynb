{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "torch.Size([800000, 3])\n",
      "SparseTensor(row=tensor([     0,      0,      0,  ..., 799999, 799999, 799999], device='cuda:0'),\n",
      "             col=tensor([  2905,  22229,  41272,  ..., 751001, 782571, 799218], device='cuda:0'),\n",
      "             val=tensor([0.1660, 0.1672, 0.1683,  ..., 0.1664, 0.1682, 0.1650], device='cuda:0'),\n",
      "             size=(800000, 800000), nnz=4800000, density=0.00%)\n",
      "DataBatch(x=[800000, 3], pos=[800000, 3], test_idx=[40000], adj_t=[800000, 800000, nnz=4800000], batch=[800000], ptr=[9])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.datasets import PCPNetDataset\n",
    "from torch_geometric.transforms import ToSparseTensor, KNNGraph, Compose\n",
    "from normal_diffusion.data.patches import PatchDataloader\n",
    "from normal_diffusion.data.transforms import DistanceToEdgeWeight, KeepNormals\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# Choose the root directory where you want to save the dataset\n",
    "root = \"../data/PCPNetDataset\"\n",
    "dataset = PCPNetDataset(\n",
    "    root=root,\n",
    "    category=\"NoNoise\",\n",
    "    split=\"train\",\n",
    "    transform=Compose([KeepNormals(), KNNGraph(k=6), DistanceToEdgeWeight(), ToSparseTensor()]),\n",
    ")\n",
    "# dataloader = PatchDataloader(dataset, batch_size=256, hops=15, transform=Compose([DistanceToEdgeWeight(), ToSparseTensor()]), limit_num_batches=1000) # can add ToSparseTensor conversion here \n",
    "# dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# dataloader = Batch.from_data_list(dataset)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "print(len(dataloader))\n",
    "first_collection = next(iter(dataloader))\n",
    "first_collection = first_collection.to(device)\n",
    "print(first_collection.x.shape)\n",
    "print(first_collection.adj_t)\n",
    "print(first_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4871, -0.8573, -0.5428],\n",
      "        [-1.2113, -0.8346, -0.2968],\n",
      "        [-1.1594, -0.7880, -0.3237],\n",
      "        ...,\n",
      "        [-1.2974, -3.5972, -1.1777],\n",
      "        [-1.8986, -3.8313, -1.3911],\n",
      "        [-1.5752, -4.1492, -1.0716]], device='cuda:0', grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from normal_diffusion.models import GCNModel\n",
    "model = GCNModel().to(device)\n",
    "t = torch.ones(first_collection.x.shape[0]).to(device)\n",
    "predicted_normals = model(graph_data=first_collection, t=t)\n",
    "print(predicted_normals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nirg/anaconda3/envs/normal-prediction/lib/python3.11/site-packages/diffusers/configuration_utils.py:134: FutureWarning: Accessing config attribute `num_train_timesteps` directly via 'DDPMScheduler' object attribute is deprecated. Please access 'num_train_timesteps' over 'DDPMScheduler's config object instead, e.g. 'scheduler.config.num_train_timesteps'.\n",
      "  deprecate(\"direct config name access\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: -0.0150\n",
      "Epoch 2/100, Loss: -0.0309\n",
      "Epoch 3/100, Loss: -0.0461\n",
      "Epoch 4/100, Loss: -0.0610\n",
      "Epoch 5/100, Loss: -0.0757\n",
      "Epoch 6/100, Loss: -0.0907\n",
      "Epoch 7/100, Loss: -0.1062\n",
      "Epoch 8/100, Loss: -0.1226\n",
      "Epoch 9/100, Loss: -0.1405\n",
      "Epoch 10/100, Loss: -0.1606\n",
      "Epoch 11/100, Loss: -0.1830\n",
      "Epoch 12/100, Loss: -0.2090\n",
      "Epoch 13/100, Loss: -0.2364\n",
      "Epoch 14/100, Loss: -0.2650\n",
      "Epoch 15/100, Loss: -0.2876\n",
      "Epoch 16/100, Loss: -0.3035\n",
      "Epoch 17/100, Loss: -0.3175\n",
      "Epoch 18/100, Loss: -0.3317\n",
      "Epoch 19/100, Loss: -0.3444\n",
      "Epoch 20/100, Loss: -0.3501\n",
      "Epoch 21/100, Loss: -0.3570\n",
      "Epoch 22/100, Loss: -0.3666\n",
      "Epoch 23/100, Loss: -0.3744\n",
      "Epoch 24/100, Loss: -0.3793\n",
      "Epoch 25/100, Loss: -0.3825\n",
      "Epoch 26/100, Loss: -0.3869\n",
      "Epoch 27/100, Loss: -0.3913\n",
      "Epoch 28/100, Loss: -0.3964\n",
      "Epoch 29/100, Loss: -0.4018\n",
      "Epoch 30/100, Loss: -0.4063\n",
      "Epoch 31/100, Loss: -0.4109\n",
      "Epoch 32/100, Loss: -0.4162\n",
      "Epoch 33/100, Loss: -0.4201\n",
      "Epoch 34/100, Loss: -0.4251\n",
      "Epoch 35/100, Loss: -0.4299\n",
      "Epoch 36/100, Loss: -0.4325\n",
      "Epoch 37/100, Loss: -0.4360\n",
      "Epoch 38/100, Loss: -0.4395\n",
      "Epoch 39/100, Loss: -0.4432\n",
      "Epoch 40/100, Loss: -0.4459\n",
      "Epoch 41/100, Loss: -0.4490\n",
      "Epoch 42/100, Loss: -0.4518\n",
      "Epoch 43/100, Loss: -0.4544\n",
      "Epoch 44/100, Loss: -0.4587\n",
      "Epoch 45/100, Loss: -0.4622\n",
      "Epoch 46/100, Loss: -0.4658\n",
      "Epoch 47/100, Loss: -0.4683\n",
      "Epoch 48/100, Loss: -0.4710\n",
      "Epoch 49/100, Loss: -0.4745\n",
      "Epoch 50/100, Loss: -0.4784\n",
      "Epoch 51/100, Loss: -0.4829\n",
      "Epoch 52/100, Loss: -0.4866\n",
      "Epoch 53/100, Loss: -0.4903\n",
      "Epoch 54/100, Loss: -0.4936\n",
      "Epoch 55/100, Loss: -0.4971\n",
      "Epoch 56/100, Loss: -0.5008\n",
      "Epoch 57/100, Loss: -0.5059\n",
      "Epoch 58/100, Loss: -0.5086\n",
      "Epoch 59/100, Loss: -0.5118\n",
      "Epoch 60/100, Loss: -0.5169\n",
      "Epoch 61/100, Loss: -0.5203\n",
      "Epoch 62/100, Loss: -0.5242\n",
      "Epoch 63/100, Loss: -0.5279\n",
      "Epoch 64/100, Loss: -0.5309\n",
      "Epoch 65/100, Loss: -0.5349\n",
      "Epoch 66/100, Loss: -0.5396\n",
      "Epoch 67/100, Loss: -0.5424\n",
      "Epoch 68/100, Loss: -0.5467\n",
      "Epoch 69/100, Loss: -0.5497\n",
      "Epoch 70/100, Loss: -0.5528\n",
      "Epoch 71/100, Loss: -0.5569\n",
      "Epoch 72/100, Loss: -0.5601\n",
      "Epoch 73/100, Loss: -0.5639\n",
      "Epoch 74/100, Loss: -0.5654\n",
      "Epoch 75/100, Loss: -0.5692\n",
      "Epoch 76/100, Loss: -0.5716\n",
      "Epoch 77/100, Loss: -0.5737\n",
      "Epoch 78/100, Loss: -0.5757\n",
      "Epoch 79/100, Loss: -0.5787\n",
      "Epoch 80/100, Loss: -0.5808\n",
      "Epoch 81/100, Loss: -0.5826\n",
      "Epoch 82/100, Loss: -0.5851\n",
      "Epoch 83/100, Loss: -0.5872\n",
      "Epoch 84/100, Loss: -0.5895\n",
      "Epoch 85/100, Loss: -0.5904\n",
      "Epoch 86/100, Loss: -0.5929\n",
      "Epoch 87/100, Loss: -0.5960\n",
      "Epoch 88/100, Loss: -0.5969\n",
      "Epoch 89/100, Loss: -0.6000\n",
      "Epoch 90/100, Loss: -0.6012\n",
      "Epoch 91/100, Loss: -0.6030\n",
      "Epoch 92/100, Loss: -0.6052\n",
      "Epoch 93/100, Loss: -0.6077\n",
      "Epoch 94/100, Loss: -0.6108\n",
      "Epoch 95/100, Loss: -0.6149\n",
      "Epoch 96/100, Loss: -0.6205\n",
      "Epoch 97/100, Loss: -0.6296\n",
      "Epoch 98/100, Loss: -0.6512\n",
      "Epoch 99/100, Loss: -0.6672\n",
      "Epoch 100/100, Loss: -0.6640\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from diffusers import DDPMScheduler\n",
    "from normal_diffusion.training.training import train_diffusion\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "scheduler = DDPMScheduler(num_train_timesteps=5, beta_schedule=\"squaredcos_cap_v2\", clip_sample=False)\n",
    "# Setup TensorBoard\n",
    "log_dir = \"logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "train_diffusion(model=model, dataloader=dataloader, scheduler=scheduler, n_epochs=100, lr=1e-3, writer=writer, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "normal-prediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
